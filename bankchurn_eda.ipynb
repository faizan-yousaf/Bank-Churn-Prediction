{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Bank Churn EDA**:\n",
    "A Comparative Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div >\n",
    "  <img src=\"https://res.cloudinary.com/dhditogyd/image/upload/v1738068132/Data%20Science%20Media/Bank%20Churn%20Prediction/l5s9r0ttmsczsblljswh.png\" width=\"100%\" height=\" 100%;\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Author: [Muhammad Faizan](https://www.linkedin.com/in/mrfaizanyousaf/)\n",
    "\n",
    "<div >\n",
    "  <img src=\"https://res.cloudinary.com/dhditogyd/image/upload/v1735402856/Passport_photo_jsvsip.png\" width=\"20%\" height=\" 20%;\"/>\n",
    "</div>\n",
    "\n",
    "## Muhammad Faizan\n",
    "\n",
    "ğŸ“ **3rd Year BS Computer Science** student at the **University of Agriculture, Faisalabad**  \n",
    "ğŸ’» Enthusiast in **Machine Learning, Data Engineering, and Data Analytics**\n",
    "\n",
    "\n",
    "## ğŸŒ Connect with Me\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/faizanyousafonly/) | [LinkedIn](https://www.linkedin.com/in/mrfaizanyousaf/) | [GitHub](https://github.com/faizan-yousaf/)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¬ Contact Me\n",
    "- **Email:** faizanyousaf815@gmail.com\n",
    "- **WhatsApp:** [+92 306 537 5389](https://wa.me/923065375389)\n",
    "\n",
    "\n",
    "ğŸ”— **Letâ€™s Collaborate:**  \n",
    "I'm always open to queries, collaborations, and discussions. Let's build something amazing together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Data (About Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context: \n",
    "This is a multivariate type of dataset which means providing or involving a variety of separate mathematical or statistical variables, multivariate numerical data analysis. It is composed of `14 attributes` which are age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise-induced angina, oldpeak â€” ST depression induced by exercise relative to rest, the slope of the peak exercise ST segment, number of major vessels and Thalassemia. This database includes `76 attributes`, but all published studies relate to the use of a subset of 14 of them. The Cleveland database is the only one used by ML researchers to date. One of the major tasks on this dataset is to predict based on the given attributes of a patient that whether that particular person has heart disease or not and other is the experimental task to diagnose and find out various insights from this dataset which could help in understanding the problem more.\n",
    "\n",
    "### Content:\n",
    "\n",
    "#### Column Descriptions:\n",
    "\n",
    "* `id:` (Unique id for each patient)\n",
    "* `age:` (Age of the patient in years)\n",
    "* `origin:` (place of study)\n",
    "* `sex:` (Male/Female)\n",
    "* `cp:` chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])\n",
    "* `trestbps:` resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))\n",
    "* `chol:` (serum cholesterol in mg/dl)\n",
    "* `fbs:` (if fasting blood sugar > 120 mg/dl)\n",
    "* `restecg:` (resting electrocardiographic results)\n",
    "* `-- Values`: [normal, stt abnormality, lv hypertrophy]\n",
    "* `thalach`: maximum heart rate achieved\n",
    "* `exang`: exercise-induced angina (True/ False)\n",
    "* `oldpeak`: ST depression induced by exercise relative to rest\n",
    "* `slope`: the slope of the peak exercise ST segment\n",
    "* `ca`: number of major vessels (0-3) colored by fluoroscopy\n",
    "* `thal`: a blood disorder called *thalassemia* [normal; fixed defect; reversible defect]\n",
    "* `num`: the predicted attribute\n",
    "\n",
    "\n",
    "### Acknowledgements\n",
    "### Creators:\n",
    "\n",
    "* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "\n",
    "### Relevant Papers:\n",
    "* Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J., Sandhu, S., Guppy, K., Lee, S., & Froelicher, V. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. American Journal of Cardiology, 64,304--310. [Web Link](http://rexa.info/paper/b884ce2f4aff7ed95ce7bfa7adabaef46b88c60c)\n",
    "* David W. Aha & Dennis Kibler. \"Instance-based prediction of heart-disease presence with the Cleveland database.\" [Web Link](rexa.info/paper/0519d1408b992b21964af4bfe97675987c0caefc)\n",
    "* Gennari, J.H., Langley, P, & Fisher, D. (1989). Models of incremental concept formation. Artificial Intelligence, 40, 11--61. [Web Link](http://rexa.info/paper/faecfadbd4a49f6705e0d3904d6770171b05041f)\n",
    "\n",
    "### Citation Request:\n",
    "The authors of the databases have requested that any publications resulting from the use of the data include the names of the principal investigator responsible for the data collection at each institution. \n",
    "\n",
    "**They would be**:\n",
    "\n",
    "* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims and Objectives:\n",
    "\n",
    "We will fill this after doing the EDA and Data Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries:**\n",
    "\n",
    "Let's start the project by importing all the libraries that we will use in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries:\n",
    "\n",
    "# 1. to handel the data:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 2. to visualize the data:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# 3. to preprocess the data:\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# 4. to build the model:\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# 5. for classification task:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 6. Metrics:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, f1_score , classification_report, root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# 7. to ignore the warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Libraries have been loaded successfully\")\n",
    "\n",
    "# 8. Display all rows and columns:\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Setting a style for the plots\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ğŸš€ Step 1: Loading the Data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading the dataset... ğŸ•µï¸â€â™‚ï¸\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting a style for the plots\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ğŸ“Š Loading and Peeking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Step 1: Loading the Data\n",
    "print(\"Loading the dataset... ğŸ•µï¸â€â™‚ï¸\")\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the dataset\n",
    "print(\"\\nLet's take a peek at the dataset: ğŸ‘€\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"\\nğŸ“ Quick summary of our dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: ğŸ”Understanding the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnderstanding the dataset structure and dimensions: ğŸ§±\")\n",
    "print(f\"Rows: {data.shape[0]}, Columns: {data.shape[1]}\")\n",
    "print(\"\\nColumn Information:\")\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§® Let's crunch some numbers!\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§® Let's crunch some numbers!\")\n",
    "print(df.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
